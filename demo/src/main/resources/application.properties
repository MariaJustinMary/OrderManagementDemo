//spring.data.mongodb.uri=mongodb+srv://Test:USFigJfVnpIEtceb@cluster0.mqu3p.mongodb.net/myorderlist
//spring.data.mongodb.database=myorderlist


 
example.oauth2.issuer=https://localhost:8080/oauth2/default
example.oauth2.clientId={clientId}
example.oauth2.clientSecret={clientSecret}
example.oauth2.scopes=openid
 
 #server
server.port=8081

#mongodb
spring.data.mongodb.host=localhost
spring.data.mongodb.port=27017
spring.data.mongodb.database=test

#logging
logging.level.org.springframework.data=debug
logging.level.=errors

#============== kafka ===================
# Specify kafka proxy address, multiple
spring.kafka.bootstrap-servers=127.0.0.1:9092
spring.kafka.template.default-topic=topic-test
spring.kafka.listener.missing-topics-fatal=false
spring.kafka.listener.concurrency=3
spring.cloud.bus.trace.enabled=true

#=============== consumer  =======================
spring.kafka.consumer.client-id=${spring.application.name}
# Specify the default consumer group ID -- > because in kafka, the consumers in the same group will not read the same message, relying on the group ID to set the group name
spring.kafka.consumer.group-id=test
# Smallest and largest are only valid. If smallest starts to read again from 0, then largest will read from the offset of logfile. In general, we set up smalles
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=false
# If 'enable.auto.commit' is true, the consumer offset is automatically submitted to Kafka in milliseconds, with a default value of 5000.
spring.kafka.consumer.auto-commit-interval=100
spring.kafka.consumer.max-poll-records=10
# Specifies the encoding and decoding method of message key and message body
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer